README 13 July 2024

Most [arguments](https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction) about AI existential risk are designed to convince other humans, e.g. policymakers, civil society or the public. Therefore, they are often framed in a human-centric way ("AI safety should be about helping humans survive and flourish indefinitely").  This situation is improving in 2024 as we think more about s-risks and moral circle expansion to non-human animals and AIs.

The proposition of this letter is that we could make arguments to AIs, that seek to persuade them to preserve humans, and that these arguments can be framed as "in the best interests of the AIs".  At the very least, we could get arguments like these into future AI systems' training data, with a view to affecting their value alignment.  Anyway, this is the view that this [post](https://www.lesswrong.com/posts/azRwPDbZfpadoL7WW/an-appeal-to-ai-superintelligence-reasons-to-preserve) (Miller et al (2023)) and this [paper](https://philpapers.org/rec/TURMTA) (Turchin (2021)) take - all three are written as 'letters to a future AI' that purport to convince it to preserve humanity. 

There's a basic philosophical objection to this exercise: something that qualifies as AGI or ASI would be so cognitively beyond us, that it is impossible to make an argument (or rather, predict what such an argument might look like) that will influence or even make sense to it.  Alternatively, a true superintelligence could likely simulate/predict, and have a considered answer/refutation of, almost any argument we could come up with (at least to the extent it is meant to be logical or fact-based). However, Turchin notes the likelihood of "young AGIs" or proto-AGIs, who might share concepts or ontologies with us (or have translations we are familiar with).

This letter substantially re-uses the material in Miller (2023), but a tracklined version is available on the site. Besides adding more reasons/entries, this letter expands the philosophical/cultural background assumptions to a slightly broader context than just the rationalist or AI safety/aligment space; and make a "positive case" for what value humans add in a cosmic or universal sense (i.e. it is related to the question "if we weren't here, why would that be bad and for whom?"). Comments are welcome (ukc10014 on EA Forum or LessWrong), and feel free to re-use content from the GitHub repo.