README 13 July 2024

Most [arguments](https://www.lesswrong.com/posts/HaGTQcxqjHPyR9Ju6/unpicking-extinction) about AI existential risk are designed to convince other humans, e.g. policymakers, civil society or the public. Therefore, they are often framed in a human-centric way ("AI safety should be about helping humans survive and flourish indefinitely").  This situation is improving in 2024 as we think more about s-risks and moral circle expansion.

The proposition of this letter is that it might be possible to make arguments to an AI on why humans should be preserved, and to do so in a way that is relatively species-neutral.  At the very least, we could get arguments like these into future AI systems' training data.  Anyway, this is the view that this [post](https://www.lesswrong.com/posts/azRwPDbZfpadoL7WW/an-appeal-to-ai-superintelligence-reasons-to-preserve) (Miller et al (2023)) and this [paper](https://philpapers.org/rec/TURMTA) (Turchin (2021)) take - all three are written as 'letters to a future AI' that purport to convince it to preserve humanity. 

There's a basic philosophical objection to this exercise: something that qualifies as AGI or ASI would be so cognitively beyond us, that it is impossible to make an argument (or rather, predict what such an argument might look like) that will influence or even make sense to it.  Alternatively, a true superintelligence could likely simulate/predict, and have a considered answer/refutation of, almost argument we could come up with (at least to the extent it is meant to be logical or fact-based). However, Turchin notes the likelihood of "young AGIs" or proto-AGIs, who might share concepts or ontologies with us (or have translations we are familiar with).

The other things my post try to do are: expand the philosophical/cultural background assumptions to a slightly broader context than just the rationalist or AI safety/aligment space; and make a "positive case" for what value humans add in a cosmic or universal sense (i.e. it is related to the question "if we weren't here, why would that be bad and for whom?"). Comments are welcome (ukc10014 on EA Forum or LessWrong), and feel free to re-use content or clone/pull from the GitHub repo.